## 背景介绍

建了仓库，地址在这里:

https://github.com/DA-southampton/ner

点star 不迷路，相关文章在github上更新的会更频繁一点QAQ

之前做过一段时间的命名体识别，项目背景其实也很简单，就是我要做一个关键词匹配的功能，第一步我需要挖掘关键词。数据调研之后发现对于一部分领域文本，比如说娱乐领域，明星领域，财经领域等等吧，这些领域的文本很有特色，一般人名/地名/公司名称/书名/电影名称都可以很好的表示文本关键信息。

在这种项目背景之下，很自然的就会想到使用命名体识别。我把在做这个项目的过程中，积累的一些资料总结了一下，希望对大家有所帮助。

关于命名体识别，这是个很大的领域，要是做好，有很多工作要做。标题完全是为了能增加曝光，自己还是知道只是一个小学生，我会把自己看过的有用的东西都列出来，给大家提供一些先验信息。

之后看到的关于nert的文章会在此基础继续更新（最近存了好多新文章还没看/苦逼码农/QAQ），不过建议大家star一下Github，不迷路，我给自己的计划是精读一些论文和博客，做一些思维导图，复现一些代码，我会努力的。

微信公众号: NLP从入门到放弃

（我果然是个渣渣，公众号名字都这么渣....欢迎关注）

## 经验介绍

对于命名体识别的代码这一块，我大概的经验就是，工作中很少直接就上复杂模型，一般都是先来简单模型，然后在优化迭代。我给个大概的方向（大家视情况而定）：

词典匹配-->HMM/CRF-->BiLSTM-CRF-->Bert系列

一般来说词典匹配是最简单的，也是最快的。不过很依赖于你的词典情况。一般来说，词典的补充需要你自己搞定，比如找相关的运营人员/产品人员，因为他们比较靠近一线工作，手上会积累一些相关的词典。或者使用合法爬虫手段（至于如何合法就自己考虑吧）去专业的垂直领域网站获取数据补充词典。

我大概分为两个个模块，第一个是各种模型的代码实现相关资源，第二个就是关于命名体识别基础知识之类的相关资源

## 代码实现

代码不再多，把一个反复看，看懂了，自己能写出来做二次开发就可以，不要今天看一个代码明天换一个代码看（小声嘟囔）



| [Bert系列 (Bert/Albert-softmax/CRF/Span/Span+focal_loss/Span+label_smoothing)做命名体识别](https://github.com/lonePatient/BERT-NER-Pytorch) | 仓库下面有Bert系列完成命名体识别的效果对比（一般来说看F1就可以）以及训练时间之类的比较，很推荐大家去看一看 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [BiLSTM-CRF实现命名体识别(Pytorch版本)](https://github.com/yanwii/ChinsesNER-pytorch?files=1) | BiLSTM-CRF我就推荐这一个吧，其他的都是大同小异，大家可以一步步去调试，做二次开发就可以，比如换个损失函数之类的。 |
| [NLP实战-中文命名实体识别-HMM/CRF 代码的实现](https://zhuanlan.zhihu.com/p/61227299) | (引用原文)本文章将通过pytorch作为主要工具实现不同的模型（包括HMM，CRF，Bi-LSTM，Bi-LSTM+CRF）来解决中文命名实体识别问题，文章不会涉及过多的数学推导，但会从直观上简单解释模型的原理，主要的内容会集中在代码部分。 |
| [隐马尔可夫模型命名实体识别NER-HMM-1](https://www.bilibili.com/video/BV1MJ411w7xR?from=search&seid=10101366636483430700)  [[隐马尔可夫模型命名实体识别NER-HMM-2](https://www.bilibili.com/video/BV1uJ411u7Ut) | 不愿意看书想看视频的同学可以看一下这个，B站首页偶然推荐给我的（推荐算法精准石锤了），讲的确实好 |
| [双向最大匹配和实体标注：你以为我只能分词？](https://zhuanlan.zhihu.com/p/133532494)------这个是词典方法命名体识别 | 这个作者总结了自己实体词典+jieba词性标注进行实体自动打标，有Python代码实现，大家可以关注一下这个博主，名字叫“叫我NLPer”，行文很有意思 |

基本上代码，我觉的看上面几个就够了吧，反复咂摸一下。



## 博客讲解

有些时候看到有人说，要想对某个概念真正有所了解，一定要看原论文。这句话肯定没错，但是不是有些时候没时间看论文吗（哭了苦逼码农）。
而且有些博客讲的是真的好啊。我大概罗列一些我局的真心不错的文章，主要就是HMM/CRF/Bilstm-CRF

| [概率图模型体系：HMM、MEMM、CRF](https://zhuanlan.zhihu.com/p/33397147) | 这个文章传播的比较广，讲的确实比较详细，不过大佬写的有些地方还是有些小问题，大家自己去挖掘吧。。。 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [最通俗易懂的BiLSTM-CRF模型中的CRF层介绍-孙孙的文章](https://zhuanlan.zhihu.com/p/44042528) | 这个文章讲的是对CRF模型的讲解，翻译的外文，原文很精彩，看译文也可以。大概讲一下，在看的过程中，要多琢磨。比如CRF有个特点全局归一化，这是区别于MEMM模型的；比如在代码实现的时候，我们一般都是使用log，所以乘法会对应加法，这样你在看源代码时候就不会懵逼；比如CRF损失函数有两个部分组成，分别有啥作用。 |
| [ner自动化打标方法](https://zhuanlan.zhihu.com/p/133532494)  | 叉烧大佬讲了一下如何用词典+最大逆向匹配做命名体识别整体思路，代码的实现可以参考第一部分那个Python代码 |
| [中文NER任务实验小结报告——深入模型实现细节](https://zhuanlan.zhihu.com/p/103779616) | 作者写了一下自己在做命名体识别的时候针对Bert的优化:BERT+CE_loss;BERT+lstmcrf;尝试用更少的标签列表;对损失函数进行了优化尝试(解决类别不平衡，因为O类别太多了);BERT+MRC;(改天我可能要精读一下，大佬写了很多内容，感觉有很多细节可以挖) |
| [如何通俗地讲解 viterbi 算法？](https://www.zhihu.com/question/20136144/answer/763021768) | 讲解了维特比算法，维特比一般是用于解码                       |
| [小标注数据量下自然语言处理实战经验](https://www.jiqizhixin.com/articles/2019-08-16-6) | 小标注数据如何处理                                           |


